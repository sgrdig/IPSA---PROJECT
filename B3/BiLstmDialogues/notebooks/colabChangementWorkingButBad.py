# -*- coding: utf-8 -*-
"""prepro&EDA copy 3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Nlwz-qyL2PqzxY0faDMcu7EpsiSXjsaU
"""

import os
import string
import numpy as np
import re
from sklearn.model_selection import train_test_split
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential,Model
from tensorflow.keras.layers import Embedding, SimpleRNN,Dropout, Dense,Input, LSTM, AdditiveAttention, Concatenate, Masking
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.callbacks import EarlyStopping
from google.colab import drive
from tensorflow.keras.layers import Embedding, SimpleRNN,Dropout, Dense,Input, LSTM, AdditiveAttention, Concatenate, Masking, Bidirectional ,BatchNormalization # Import Bidirectional

"""# Chargement des Données"""

drive.mount('/content/drive')
dataPath = "/content/drive/My Drive/Colab Notebooks/Data/"
linesFile = os.path.join(dataPath, "movie_lines.txt")
print(linesFile)
conversationsFile = os.path.join(dataPath, "movie_conversations.txt")
print(conversationsFile)

lines = open(linesFile, encoding='utf-8', errors='ignore').read().split('\n')
conversations = open(conversationsFile, encoding='utf-8', errors='ignore').read().split('\n')

def lowercase(text):
    if isinstance(text, list):
        return [t.lower() for t in text if isinstance(t, str)]
    elif isinstance(text, str):
        return text.lower()
    else:
        raise ValueError("Input should be a string or a list of strings.")

def clean_text(text):
    text = re.sub(r"\b(?![aI])[a-zA-Z]\b|\\", "", text)
    text = re.sub(r"\d+", "", text)
    text = re.sub(r"[^\w\s]", "", text)
    text = re.sub(r"\s+", " ", text).strip()
    return text

line_dict = {}
with open(linesFile, 'r', encoding='ISO-8859-1') as file:
    for line in file:
        parts = line.strip().split(' +++$+++ ')
        if len(parts) == 5:
            lineID = parts[0]
            utterance = clean_text(lowercase(parts[-1]))
            line_dict[lineID] = utterance

# # Create context-response pairs from conversations
# X = []
# y = []
# with open(conversationsFile, 'r', encoding='ISO-8859-1') as file:
#     for line in file:
#         parts = line.strip().split(' +++$+++ ')
#         if len(parts) == 4:  # Ensure correct format
#             lineIDs = eval(parts[-1])  # Convert string list to Python list
#             for i in range(len(lineIDs) - 1):
#                 # Use the current line as context and the next line as response
#                 if lineIDs[i] in line_dict and lineIDs[i + 1] in line_dict:
#                     X.append(line_dict[lineIDs[i]])  # Context
#                     y.append(line_dict[lineIDs[i + 1]])  # Response

start_token = "<start>"
end_token = "<end>"

X = []  # Contexte
y = []  # Réponse

with open(conversationsFile, 'r', encoding='ISO-8859-1') as file:
    for line in file:
        parts = line.strip().split(' +++$+++ ')
        if len(parts) == 4:
            lineIDs = eval(parts[-1])
            for i in range(len(lineIDs) - 1):
                if lineIDs[i] in line_dict and lineIDs[i + 1] in line_dict:
                    X.append(line_dict[lineIDs[i]])
                    response = start_token + " " + line_dict[lineIDs[i + 1]] + " " + end_token
                    y.append(response)

X =  X[:75000]
y = y[:75000]

X[0] , y[0]

max_vocab_size = 10000  # Maximum number of words in the vocabulary
max_sequence_length = 40  # Maximum sequence length

tokenizer = Tokenizer(num_words=max_vocab_size, oov_token="<OOV>")

texts_with_special_tokens = X + y

tokenizer.fit_on_texts(texts_with_special_tokens)

word_index = tokenizer.word_index
if '<start>' not in word_index:
    word_index['<start>'] = len(word_index) + 1
if '<end>' not in word_index:
    word_index['<end>'] = len(word_index) + 1

tokenizer.word_index = word_index

X_sequences = tokenizer.texts_to_sequences(X)
y_sequences = tokenizer.texts_to_sequences(y)

X_sequences

X_padded = pad_sequences(X_sequences, maxlen=max_sequence_length, padding='post', truncating='post')
y_padded = pad_sequences(y_sequences, maxlen=max_sequence_length, padding='post', truncating='post')



X_padded

vocab_size = len(tokenizer.word_index) + 1

decoder_targets = np.expand_dims(y_padded, axis=-1)

embedding_dim = 256
units = 128

batch_size = 265
epochs = 30



encoder_inputs = Input(shape=(max_sequence_length,))
encoder_masking = Masking(mask_value=0)(encoder_inputs)  # Mask out padding (value 0)
encoder_embedding = Embedding(vocab_size, embedding_dim)(encoder_masking)
encoder_bilstm, forward_h, forward_c, backward_h, backward_c = Bidirectional(
    LSTM(units, return_state=True)
)(encoder_embedding)

encoder_state_h = Concatenate()([forward_h, backward_h])
encoder_state_c = Concatenate()([forward_c, backward_c])
encoder_states = [encoder_state_h, encoder_state_c]

decoder_inputs = Input(shape=(max_sequence_length,))
decoder_masking = Masking(mask_value=0)(decoder_inputs)
decoder_embedding = Embedding(vocab_size, embedding_dim)(decoder_masking)
encoder_embedding = Dropout(0.3)(encoder_embedding)  # Dropout pour éviter l'overfitting
encoder_embedding = BatchNormalization()(encoder_embedding)

decoder_bilstm = Bidirectional(LSTM(units, return_sequences=True , dropout=0.3, recurrent_dropout=0.3))(decoder_embedding)
decoder_dense = Dense(vocab_size, activation='softmax')(decoder_bilstm)

bilstm_model = Model([encoder_inputs, decoder_inputs], decoder_dense)
bilstm_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

bilstm_model.summary()

bilstm_model.fit([X_padded, y_padded], decoder_targets, batch_size=batch_size, epochs=30, validation_split=0.2)

# prompt: save le model sur collab et sur mpn pc

# Save the model to Google Drive
model_save_path = "/content/drive/My Drive/Colab Notebooks/Data/bilstm_model.h5"
bilstm_model.save(model_save_path)

# Save the model locally (in the Colab environment)
local_model_path = "/content/bilstm_model.h5" # Or choose any path
bilstm_model.save(local_model_path)


# Download the model from Colab to your local machine

# prompt: save sur mon pc le model en H5

from google.colab import files
files.download('/content/bilstm_model.h5')

print(tokenizer.word_index.get("roxanne"))
print(tokenizer.word_index.get("korrine"))
print(tokenizer.word_index.get("andrew"))
print(tokenizer.word_index.get("can"))
print(tokenizer.word_index.get("we"))
print(tokenizer.word_index.get("make"))
print(tokenizer.word_index.get("this"))
print(tokenizer.word_index.get("quick"))

def chatbot_response(input_text):
    input_seq = tokenizer.texts_to_sequences([input_text])
    input_seq = pad_sequences(input_seq, maxlen=max_sequence_length, padding="post")

    target_seq = np.zeros((1, max_sequence_length))

    decoded_sentence = []

    for i in range(max_sequence_length):
        preds = bilstm_model.predict([input_seq, target_seq])

        print(f"Step {i}: {preds[0, i, :]}")
        next_word_id = np.argmax(preds[0, i, :])

        if next_word_id == 0:
            break

        decoded_sentence.append(tokenizer.index_word[next_word_id])
        target_seq[0, i] = next_word_id

    return " ".join(decoded_sentence)

question = "can we make this quick roxanne korrine and andrew barrett are having an incredibly horrendous public break up on the quad again"
response = chatbot_response(question)
print(f"Question: {question}")
print(f"Réponse: {response}")

def chatbot_response(input_text):
    input_seq = tokenizer.texts_to_sequences([input_text])
    input_seq = pad_sequences(input_seq, maxlen=max_sequence_length, padding="post")

    target_seq = np.zeros((1, max_sequence_length))
    target_seq[0, 0] = tokenizer.word_index.get("<start>", 1)  # Token de départ

    decoded_sentence = []

    for i in range(max_sequence_length - 1):  # Empêche le dépassement
        preds = bilstm_model.predict([input_seq, target_seq[:, :i+1]])  # Prédiction itérative
        next_word_id = np.argmax(preds[0, -1, :])  # Prend le dernier mot généré

        if next_word_id == tokenizer.word_index.get("<end>", 0):  # Vérifie le token de fin
            break

        decoded_sentence.append(tokenizer.index_word.get(next_word_id, ""))  # Récupérer le mot

        if i + 1 < max_sequence_length:  # Vérifie avant d'écrire pour éviter l'IndexError
            target_seq[0, i+1] = next_word_id

    return " ".join(decoded_sentence)

# Test
question = "can we make this quick roxanne korrine and andrew barrett are having an incredibly horrendous public break up on the quad again"
response = chatbot_response(question)

print(f"Question: {question}")
print(f"Réponse: {response}")

# Verify special tokens in tokenizer
print("Special tokens in tokenizer:", {token: tokenizer.word_index.get(token) for token in ["<start>", "<end>", "<OOV>"]})

# Check a few training examples
print("Sample X sequence:", X_sequences[0])
print("Sample y sequence:", y_sequences[0])

def generate_response(input_text, model, tokenizer, max_sequence_length):
    # Clean and preprocess the input text
    input_text = clean_text(lowercase(input_text))

    # Convert input to sequence
    input_seq = tokenizer.texts_to_sequences([input_text])
    input_seq = pad_sequences(input_seq, maxlen=max_sequence_length, padding='post')

    # Start with a blank target sequence for the decoder
    target_seq = np.zeros((1, max_sequence_length))

    # Since <start> isn't in the tokenizer, we'll use the OOV token as our start signal
    # OOV token is at index 1
    target_seq[0, 0] = 1  # Using OOV token as start signal

    # Initialize the response text
    response_words = []

    for i in range(max_sequence_length - 1):
        # Predict next token
        output = model.predict([input_seq, target_seq], verbose=0)

        # Get the index of most likely next word
        sampled_token_index = np.argmax(output[0, i, :])

        # Exit if we predict padding (0)
        if sampled_token_index == 0:
            break

        # Get the actual word
        sampled_word = tokenizer.index_word.get(sampled_token_index, "<OOV>")

        # Add the predicted word to our response
        if sampled_word != "<OOV>":  # Skip OOV tokens in the output
            response_words.append(sampled_word)

        # Update target sequence for next prediction
        target_seq[0, i+1] = sampled_token_index

    return " ".join(response_words)

# Tester avec un exemple

input_text = "What is going on with Roxanne and Andrew?"
response = generate_response(input_text, bilstm_model, tokenizer, max_sequence_length)
print(response)

question = "What is going on with Roxanne and Andrew?"
response = generate_response(question , bilstm_model , tokenizer=tokenizer ,max_sequence_length=40)
print(f"Question: {question}")
print(f"Réponse: {response}")

import numpy as np
from tensorflow.keras.preprocessing.sequence import pad_sequences

# Fonction de décodage pour générer une réponse
def generate_response(input_text, model, tokenizer, max_sequence_length):
    # Prétraiter l'entrée
    input_seq = tokenizer.texts_to_sequences([input_text])
    input_seq = pad_sequences(input_seq, maxlen=max_sequence_length, padding='post')

    # Initialiser la séquence de sortie
    decoder_input_seq = np.zeros_like(input_seq)  # Au début, la séquence de sortie est vide
    output_text = ""

    # Décodage séquentiel
    for _ in range(max_sequence_length):
        predictions = model.predict([input_seq, decoder_input_seq])
        predicted_index = np.argmax(predictions[0, -1, :])  # Prend le dernier mot prédit

        # Si le modèle génère un mot trop fréquent (par exemple, 0 ou des espaces)
        predicted_word = tokenizer.index_word.get(predicted_index, None)
        if not predicted_word or predicted_word == ' ':
            break

        # Ajouter le mot prédit à la sortie
        output_text += " " + predicted_word

        # Mettre à jour la séquence de sortie (décodeur)
        decoder_input_seq = np.zeros_like(input_seq)
        decoder_input_seq[0, 0] = predicted_index  # Mettre à jour le dernier mot généré

    return output_text

# Tester avec un exemple
response = generate_response("What is going on with Roxanne and Andrew?", bilstm_model, tokenizer, max_sequence_length)
print(response)