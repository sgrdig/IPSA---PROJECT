# -*- coding: utf-8 -*-
"""prepro&EDA.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14b2-721L1WQGjac07DWsdasIEELI4-Cf
"""

#Imports
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import nltk
import os
import re
import ast
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from nltk.corpus import stopwords
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Bidirectional, Dense, Input
from google.colab import drive
nltk.download('stopwords')
stop_words = set(stopwords.words('french'))

"""# Chargement des Données"""

drive.mount('/content/drive')
dataPath = "/content/drive/My Drive/Colab Notebooks/Data/"
linesFile = os.path.join(dataPath, "movie_lines.txt")
print(linesFile)
conversationsFile = os.path.join(dataPath, "movie_conversations.txt")
print(conversationsFile)

!ls "/content/drive/My Drive/Colab Notebooks/Data/"

# dataPath =  "../data/"
# linesFile = os.path.join(dataPath, "movie_lines.txt")
# conversationsFile = os.path.join(dataPath, "movie_conversations.txt")

with open(linesFile, 'r' , encoding='latin-1') as f:
    lines = f.readlines()

with open(conversationsFile, 'r') as f:
    conversations = f.readlines()

lines[:5]

conversations[:5]

"""# Transformation des Données"""

movieLines  = {}

for lignes in lines:
    ligne = lignes.split(" +++$+++ ")
    if len(ligne) == 5:
        movieLines[ligne[0]] = ligne[4]

for key, value in list(movieLines.items())[:5]:
    print(f"{key}: {value}")

movieConversations = []
for conv in conversations:
    parts = conv.strip().split(" +++$+++ ")
    try:
        conversation_ids = ast.literal_eval(parts[3])
        movieConversations.append(conversation_ids)
    except:
        pass

movieConversations

movieConversationsWithText = []
for conversation in movieConversations:
    conversationWithText = []
    for line in conversation:
        if line in movieLines:
            conversationWithText.append(movieLines[line])
    movieConversationsWithText.append(conversationWithText)

movieConversationsCleaned = [
    [line.strip() for line in conversation]
    for conversation in movieConversationsWithText
]

movieConversationsCleaned

"""## Passage en Dataframe"""

pairs = []
for conversation in movieConversationsCleaned:
    for i in range(len(conversation) - 1):
        pairs.append((conversation[i], conversation[i + 1]))

df = pd.DataFrame(pairs, columns=["intro", "ccl"])

print(df.head(1))
print(df.info())
print(df.describe())
print(df.columns)
df

"""# Cleaning"""

def clean_text(text):
    text = text.lower()
    text = re.sub(r"[^a-zA-Z0-9' ]", "", text)
    text = re.sub(r"\d+", "<num>", text)
    text = re.sub(r"(\w)'(\w)", r"\1 ' \2", text)
    text = re.sub(r"\s+", " ", text).strip()

    words = text.split()
    words = [word for word in words if word not in stop_words]

    return " ".join(words)

df["questions"] = df["intro"].apply(clean_text)
df["reponce"] = df["ccl"].apply(clean_text)

print(df.head())

vocab_size = 10000
tokenizer = Tokenizer(num_words=vocab_size, oov_token="<OOV>")

tokenizer.fit_on_texts(df['questions'].tolist() + df['reponce'].tolist())

X = tokenizer.texts_to_sequences(df['questions'].tolist())
y = tokenizer.texts_to_sequences(df['reponce'].tolist())

print(X[:3])
print(y[:3])

max_length = 40

X = pad_sequences(X, maxlen=max_length, padding='post')
y = pad_sequences(y, maxlen=max_length, padding='post')

print(f"Shape de X: {X.shape}")
print(f"Shape de y: {y.shape}")

import numpy as np

# Décalage : chaque X doit prédire le mot suivant de y
y = np.array([seq[1:] + [0] for seq in y])  # Décalage + padding
y = np.expand_dims(y, axis=-1)  # Reshape pour le modèle

print(f"Nouvelle shape de y: {y.shape}")  # (nb_exemples, max_length, 1)

y = np.array([seq[-1] for seq in y])

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

print(f"Shape de X_train: {X_train.shape}, y_train: {y_train.shape}")
print(f"Shape de X_test: {X_test.shape}, y_test: {y_test.shape}")

# prompt: setup the seed for keras also

import tensorflow as tf

# Set the seed for TensorFlow/Keras
tf.random.set_seed(42)

# Set the seed for NumPy (for other random operations)
np.random.seed(42)

from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional, BatchNormalization, Attention, LayerNormalization
from tensorflow.keras.regularizers import l2
from tensorflow.keras.callbacks import EarlyStopping

embedding_dim = 100

model = Sequential([
    Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_length),
    Bidirectional(LSTM(64, return_sequences=True)),
    Dropout(0.5),
    Bidirectional(LSTM(32)),
    Dense(vocab_size, activation='softmax' ,kernel_regularizer=l2(0.01) )
])

model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)

history = model.fit(X_train, y_train, epochs=50, batch_size=32, validation_split=0.2, callbacks=[early_stopping])

plt.figure(figsize=(8,5))
plt.plot(history.history['loss'], label='Train Loss', color='blue')
plt.plot(history.history['val_loss'], label='Validation Loss', color='red')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.title('Training Loss vs Validation Loss')
plt.legend()
plt.grid()
plt.show()

# prompt: save le model sur le drive et sur mon pc

# ... (Your existing code) ...

# Save the model to Google Drive
model.save('/content/drive/My Drive/Colab Notebooks/my_chatbot_model.h5')
print("Model saved to Google Drive.")

# Optionally, download the model to your local machine
from google.colab import files
files.download('/content/drive/My Drive/Colab Notebooks/my_chatbot_model.h5')
print("Model downloaded to your local machine.")